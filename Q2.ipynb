{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e135c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4ab23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    textnew = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for i in tokens:\n",
    "        if i not in stop_words:\n",
    "            if i.isalnum():\n",
    "                textnew.append(i)\n",
    "\n",
    "    return textnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "206fe421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get a list of all files in the directory\n",
    "directory = \"Humor,Hist,Media,Food/\"\n",
    "files = os.listdir(directory)\n",
    "pos_index = {}\n",
    "documentid = {}\n",
    "num = 1\n",
    "for file_name in files: \n",
    "    documentid[num] = file_name\n",
    "    \n",
    "    file_text = open(directory+file_name,'r',encoding='ISO-8859-1').read()    # read the file\n",
    "    tokens = preprocess(file_text)\n",
    "    \n",
    "    for pos,token in enumerate(tokens):\n",
    "        if token not in pos_index:\n",
    "            pos_index[token] = [0,{}]\n",
    "            \n",
    "        if num not in pos_index[token][1]:\n",
    "            pos_index[token][0]+=1\n",
    "            pos_index[token][1][num] = []\n",
    "            \n",
    "        pos_index[token][1][num].append(pos)\n",
    "    num+= 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aefaf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(query):\n",
    "    \n",
    "    query_token = preprocess(query)\n",
    "    for i in range(len(query_token)):\n",
    "        query_token[i] = [query_token[i],i]\n",
    "  \n",
    "    \n",
    "    for i in range(len(query_token)):\n",
    "        for j in range(len(query_token)-i-1):\n",
    "            if pos_index[query_token[i][0]][0] > pos_index[query_token[j+1][0]][0] :\n",
    "                \n",
    "                query_token[j], query_token[j + 1] = query_token[j + 1], query_token[j]\n",
    "#     print(query_token)  \n",
    "                \n",
    "#     print(query_token)\n",
    "    \n",
    "    docs = pos_index[query_token[0][0]][1]\n",
    "    qu_in = query_token[0][1]\n",
    "#     print(docs)\n",
    "    for i in query_token:\n",
    "        dock = {}\n",
    "        for docid in docs:\n",
    "            for j in pos_index[i[0]][1]:\n",
    "                if docid==j:\n",
    "                    for k in docs[docid]:\n",
    "                        f = 0\n",
    "                        for l in pos_index[i[0]][1][j]:\n",
    "                            if(k-qu_in == l-i[1]):\n",
    "                                dock[docid] = docs[docid]\n",
    "        docs = dock\n",
    "\n",
    "#     print(docs)\n",
    "    print(\"The number of found for query are:\", len(docs))\n",
    "    print(\"The document names are:\")\n",
    "    for i in docs:\n",
    "        print(documentid[i])\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f7d0dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of found for query are: 168\n",
      "The document names are:\n",
      "acronym.lis\n",
      "acronym.txt\n",
      "acronyms.txt\n",
      "ads.txt\n",
      "advrtize.txt\n",
      "age.txt\n",
      "aids.txt\n",
      "allfam.epi\n",
      "allusion\n",
      "anorexia.txt\n",
      "aphrodis.txt\n",
      "argotdic.txt\n",
      "bb\n",
      "beerjesus.hum\n",
      "beerwarn.txt\n",
      "bible.txt\n",
      "bnbeg2.4.txt\n",
      "bnbguide.txt\n",
      "boatmemo.jok\n",
      "btscke01.des\n",
      "btscke02.des\n",
      "btscke03.des\n",
      "btscke04.des\n",
      "btscke05.des\n",
      "c0dez.txt\n",
      "cake.rec\n",
      "childhoo.jok\n",
      "climbing.let\n",
      "cmu.share\n",
      "college.sla\n",
      "comic_st.gui\n",
      "commutin.jok\n",
      "confucius_say.txt\n",
      "cookie.1\n",
      "coollngo2.txt\n",
      "cuchy.hum\n",
      "cucumber.jok\n",
      "cucumber.txt\n",
      "cultmov.faq\n",
      "dead4.txt\n",
      "dead5.txt\n",
      "devils.jok\n",
      "donut.txt\n",
      "drinks.gui\n",
      "dromes.txt\n",
      "dym\n",
      "empeval.txt\n",
      "epi_.txt\n",
      "epi_bnb.txt\n",
      "eskimo.nel\n",
      "films_gl.txt\n",
      "flux_fix.txt\n",
      "gameshow.txt\n",
      "gd_flybd.txt\n",
      "gd_hhead.txt\n",
      "girlspeak.txt\n",
      "golnar.txt\n",
      "gown.txt\n",
      "grail.txt\n",
      "growth.txt\n",
      "hackmorality.txt\n",
      "headlnrs\n",
      "hi.tec\n",
      "horflick.txt\n",
      "horoscop.jok\n",
      "horoscop.txt\n",
      "horoscope.txt\n",
      "hotel.txt\n",
      "htswfren.txt\n",
      "humor9.txt\n",
      "inquirer.txt\n",
      "insanity.hum\n",
      "insult.lst\n",
      "insults1.txt\n",
      "jason.fun\n",
      "jokes\n",
      "jokes.txt\n",
      "jokes1.txt\n",
      "laws.txt\n",
      "lawsuniv.hum\n",
      "letter.txt\n",
      "limerick.jok\n",
      "lines.jok\n",
      "llong.hum\n",
      "lotsa.jok\n",
      "lozerzon.hum\n",
      "lucky.cha\n",
      "luvstory.txt\n",
      "macsfarm.old\n",
      "malechem.txt\n",
      "manners.txt\n",
      "marines.hum\n",
      "math.1\n",
      "melodram.hum\n",
      "men&wome.txt\n",
      "merry.txt\n",
      "missdish\n",
      "mlverb.hum\n",
      "montpyth.hum\n",
      "mothers.txt\n",
      "mrscienc.hum\n",
      "murphys.txt\n",
      "murphy_l.txt\n",
      "nigel.10\n",
      "nigel.2\n",
      "nigel.5\n",
      "nigel10.txt\n",
      "nukeplay.hum\n",
      "oliver02.txt\n",
      "onetoone.hum\n",
      "onetotwo.hum\n",
      "phunatdi.ana\n",
      "phxbbs-m.txt\n",
      "pickup.lin\n",
      "pickup.txt\n",
      "prac2.jok\n",
      "practica.txt\n",
      "progrs.gph\n",
      "psych_pr.quo\n",
      "quack26.txt\n",
      "quick.jok\n",
      "radexposed.txt\n",
      "rapmastr.hum\n",
      "readme.bat\n",
      "rednecks.txt\n",
      "rockmus.hum\n",
      "sf-zine.pub\n",
      "shooters.txt\n",
      "skincat\n",
      "slogans.txt\n",
      "smackjok.hum\n",
      "sorority.gir\n",
      "staff.txt\n",
      "stereo.txt\n",
      "stressman.txt\n",
      "strine.txt\n",
      "sungenu.hum\n",
      "swearfrn.hum\n",
      "taping.hum\n",
      "telecom.q\n",
      "televisi.hum\n",
      "televisi.txt\n",
      "test2.jok\n",
      "testchri.txt\n",
      "texican.dic\n",
      "texican.lex\n",
      "tfepisod.hum\n",
      "the_math.hel\n",
      "top10.txt\n",
      "top10st1.txt\n",
      "top10st2.txt\n",
      "tpquote2.txt\n",
      "tpquotes.txt\n",
      "tuflife.txt\n",
      "twinkies.jok\n",
      "twinpeak.txt\n",
      "t_zone.jok\n",
      "urban.txt\n",
      "variety1.asc\n",
      "variety2.asc\n",
      "whatthe.hum\n",
      "wkrp.epi\n",
      "wrdnws3.txt\n",
      "wrdnws5.txt\n",
      "wrdnws8.txt\n",
      "wrdnws9.txt\n",
      "x-drinks.txt\n",
      "xibovac.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"have sex\"\n",
    "phrase_query(query)\n",
    "# print(pos_index['first'][268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1199a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
